{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Graph Neural Networks (GNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahan/miniconda3/envs/specformer/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import GraphConv\n",
    "from IPython.display import Latex\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/vahan/.dgl/amazon_co_buy_photo.zip from https://data.dgl.ai/dataset/amazon_co_buy_photo.zip...\n",
      "Extracting file to /home/vahan/.dgl/amazon_co_buy_photo\n",
      "Number of classes: 8\n",
      "Number of nodes: 7650\n",
      "Number of edges: 245813\n"
     ]
    }
   ],
   "source": [
    "# Import datasat\n",
    "dataset = dgl.data.AmazonCoBuyPhotoDataset()\n",
    "print('Number of classes:', dataset.num_classes)\n",
    "\n",
    "# A DGL Dataset object may contain one or multiple graphs. The Amazon\n",
    "# dataset used in this lab only consists of one single graph.\n",
    "graph = dataset[0]\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "print('Number of nodes:', graph.num_nodes())\n",
    "print('Number of edges:', graph.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DGL graph can store node features in a\n",
    "dictionary-like attribute called ``ndata``.\n",
    "In the DGL Amazon co-buy dataset, the graph contains the following node features:\n",
    "\n",
    "- ``label``: The ground truth node category.\n",
    "\n",
    "-  ``feat``: The node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node labels and features\n",
      "{'feat': tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.]]), 'label': tensor([2, 2, 2,  ..., 3, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print('Node labels and features')\n",
    "print(graph.ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Retrieve key properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  745\n"
     ]
    }
   ],
   "source": [
    "# Define key graph variables\n",
    "X = graph.ndata['feat']\n",
    "y = graph.ndata['label']\n",
    "num_classes = dataset.num_classes\n",
    "num_feat = X.shape[1]\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "print('Number of features: ', num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(N, train_ratio, seed=4):\n",
    "    \"\"\" Creates train/val/test masks\n",
    "\n",
    "    Args:\n",
    "        N (int): dataset size\n",
    "        train_ratio (float): proportion of the training set\n",
    "        seed (int, optional): Fixes random. Defaults to 10\n",
    "\n",
    "    Return: \n",
    "        [tensors]: returns boolean tensors for train/val/test set\n",
    "        True indicates that a node belong to this set, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(train_ratio * N)\n",
    "    val_size = int((N - train_size)/2)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    # split dataset\n",
    "    subsets = torch.utils.data.random_split(range(N), lengths = [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "    train_inds, val_inds, test_inds = [torch.Tensor(subset.indices) for subset in subsets]\n",
    "\n",
    "    # create tensors of masks for each subset\n",
    "    dataset_inds = torch.arange(N)\n",
    "    train_mask = torch.isin(dataset_inds, train_inds)\n",
    "    val_mask = torch.isin(dataset_inds, val_inds)\n",
    "    test_mask = torch.isin(dataset_inds, test_inds)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "train_mask, val_mask, test_mask = split_dataset(N, train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement a Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H^{(l+1)} = f(H^{(l)}, A) = \\sigma( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./gcn_web.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Graph Convolution Network \n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_size, hidden_size, output_size, dropout):\n",
    "        super(GNN_model, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "        self.convs = torch.nn.ModuleList() # holds GraphConv layers in a list\n",
    "        self.convs.append(\n",
    "            GraphConv(input_size, hidden_size, activation=F.relu)) # You can either define the activation at the layer level or call it inside the forward\n",
    "        for i in range(num_layers-2):\n",
    "            self.convs.append(\n",
    "                GraphConv(hidden_size, hidden_size, activation=F.relu))\n",
    "        self.convs.append(GraphConv(hidden_size, output_size))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        # Implement the forward function that takes the graph,\n",
    "        # the features tensor x and returns the output tensor as shown in figure 1\n",
    "        for conv in self.convs:\n",
    "            x = conv(graph, x)\n",
    "    \n",
    "        output = F.log_softmax(x, dim=1) # Log_softmax is more stable numerically in comparison to softmax\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, x, labels, num_epochs, optimizer, train_mask, val_mask, test_mask):\n",
    "    \"\"\" Train the GNN model \n",
    "\n",
    "    Args:\n",
    "        model: GNN model defined in pytorch\n",
    "        graph (dgl.graph): dataset on which the task is performed\n",
    "        x (tensor): node feature matrix \n",
    "        labels (tensor): node labels\n",
    "        num_epochs (int): number of epochs\n",
    "        optimizer: Adam optimizer\n",
    "        train_mask (tensor): boolean mask for training nodes\n",
    "        val_mask (tensor): boolean mask for validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the model (pytorch specific)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward\n",
    "        pred = model(graph, x)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = nll_loss(pred[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}: loss {:.3f}, train Acc: {:.3f}, val acc: {:.3f}, test acc: {:.3f}'.format(\n",
    "                epoch, loss, train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate model\n",
    "num_layers=3\n",
    "hidden_size=16\n",
    "dropout=0.3\n",
    "num_epochs=300\n",
    "lr=0.01\n",
    "weight_decay=0.005\n",
    "train_ratio=0.8\n",
    "seed=4\n",
    "\n",
    "model = GNN_model(num_layers, num_feat, hidden_size, num_classes, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.203, train Acc: 0.121, val acc: 0.114, test acc: 0.119\n",
      "Epoch 10: loss 1.810, train Acc: 0.423, val acc: 0.431, test acc: 0.455\n",
      "Epoch 20: loss 1.505, train Acc: 0.428, val acc: 0.437, test acc: 0.456\n",
      "Epoch 30: loss 1.134, train Acc: 0.632, val acc: 0.633, test acc: 0.665\n",
      "Epoch 40: loss 0.761, train Acc: 0.808, val acc: 0.818, test acc: 0.829\n",
      "Epoch 50: loss 0.530, train Acc: 0.853, val acc: 0.855, test acc: 0.859\n",
      "Epoch 60: loss 0.438, train Acc: 0.894, val acc: 0.898, test acc: 0.902\n",
      "Epoch 70: loss 0.393, train Acc: 0.907, val acc: 0.902, test acc: 0.903\n",
      "Epoch 80: loss 0.356, train Acc: 0.911, val acc: 0.908, test acc: 0.911\n",
      "Epoch 90: loss 0.344, train Acc: 0.913, val acc: 0.911, test acc: 0.915\n",
      "Epoch 100: loss 0.324, train Acc: 0.918, val acc: 0.905, test acc: 0.907\n",
      "Epoch 110: loss 0.310, train Acc: 0.923, val acc: 0.915, test acc: 0.914\n",
      "Epoch 120: loss 0.314, train Acc: 0.917, val acc: 0.907, test acc: 0.919\n",
      "Epoch 130: loss 0.297, train Acc: 0.927, val acc: 0.923, test acc: 0.914\n",
      "Epoch 140: loss 0.289, train Acc: 0.926, val acc: 0.918, test acc: 0.919\n",
      "Epoch 150: loss 0.283, train Acc: 0.929, val acc: 0.920, test acc: 0.919\n",
      "Epoch 160: loss 0.279, train Acc: 0.929, val acc: 0.931, test acc: 0.923\n",
      "Epoch 170: loss 0.284, train Acc: 0.925, val acc: 0.924, test acc: 0.916\n",
      "Epoch 180: loss 0.272, train Acc: 0.931, val acc: 0.927, test acc: 0.922\n",
      "Epoch 190: loss 0.269, train Acc: 0.930, val acc: 0.924, test acc: 0.920\n",
      "Epoch 200: loss 0.301, train Acc: 0.918, val acc: 0.919, test acc: 0.919\n",
      "Epoch 210: loss 0.431, train Acc: 0.857, val acc: 0.876, test acc: 0.864\n",
      "Epoch 220: loss 0.272, train Acc: 0.931, val acc: 0.928, test acc: 0.916\n",
      "Epoch 230: loss 0.264, train Acc: 0.930, val acc: 0.928, test acc: 0.922\n",
      "Epoch 240: loss 0.261, train Acc: 0.932, val acc: 0.925, test acc: 0.925\n",
      "Epoch 250: loss 0.259, train Acc: 0.932, val acc: 0.929, test acc: 0.924\n",
      "Epoch 260: loss 0.255, train Acc: 0.932, val acc: 0.929, test acc: 0.923\n",
      "Epoch 270: loss 0.253, train Acc: 0.935, val acc: 0.932, test acc: 0.922\n",
      "Epoch 280: loss 0.262, train Acc: 0.928, val acc: 0.914, test acc: 0.916\n",
      "Epoch 290: loss 0.252, train Acc: 0.933, val acc: 0.927, test acc: 0.923\n"
     ]
    }
   ],
   "source": [
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "\n",
    "# Train model\n",
    "train(model, graph, X, y, num_epochs, optimizer, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Graph Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/vahan/.dgl/ENZYMES.zip from https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip...\n",
      "Extracting file to /home/vahan/.dgl/ENZYMES\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.TUDataset(name='ENZYMES')\n",
    "\n",
    "# Add self loop to each graph\n",
    "dataset.graph_lists = [dgl.add_self_loop(graph) for graph in dataset.graph_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Graph(num_nodes=37, num_edges=205,\n",
       "       ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
       "       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n",
       " tensor([5]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graph categories: 6\n",
      "Dimension of nodes features 18\n"
     ]
    }
   ],
   "source": [
    "print('Number of graph categories:', dataset.num_labels)\n",
    "print('Dimension of nodes features', dataset[0][0].ndata['node_attr'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "train_sampler, val_sampler, test_sampler = dgl.data.utils.split_dataset(\n",
    "        dataset, frac_list=[0.6, 0.2, 0.2], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch graphs with GraphDataLoader\n",
    "train_dataloader = GraphDataLoader(\n",
    "        train_sampler, batch_size=5, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    val_sampler, batch_size=5, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    test_sampler, batch_size=5, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Create GNN model for graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGraphModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers, input_size, hidden_size, output_size):\n",
    "        super(BasicGraphModel, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GraphConv(input_size, hidden_size))\n",
    "        for i in range(n_layers-1):\n",
    "            self.convs.append(GraphConv(hidden_size, hidden_size))\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # Message Passing -- Learn node representations via GCN\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(g, x)\n",
    "            x = F.elu(x)\n",
    "        x = self.convs[-1](g, x)\n",
    "        # Readout -- average all node representations to get graph embedding\n",
    "        g.ndata['h'] = x\n",
    "        x = dgl.mean_nodes(g, 'h')\n",
    "        # Apply linear layer to classify graph representation\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fcn, optimizer, train_dataloader, val_dataloader, num_epochs):\n",
    "    model = model.double()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch, batched_graph in enumerate(train_dataloader):\n",
    "            batched_graph, labels = batched_graph\n",
    "            logits = model(batched_graph, batched_graph.ndata['node_attr'].double())\n",
    "            loss = loss_fcn(logits, labels.T[0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.mean(losses)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch {} | Loss: {:.4f}\".format(epoch, loss_data))\n",
    "            test(model, loss_fcn, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fcn, dataloader):\n",
    "    scores = []\n",
    "    for batch, batched_graph in enumerate(dataloader):\n",
    "        batched_graph, labels = batched_graph\n",
    "        scores.append(\n",
    "            evaluate(model, batched_graph, labels, loss_fcn))\n",
    "    mean_scores = np.mean(scores)\n",
    "    print(\"Accuracy score: {:.4f}\".format(mean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batched_graph, labels, loss_fcn):\n",
    "    model = model.double()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(batched_graph, batched_graph.ndata['node_attr'].double())\n",
    "\n",
    "    labels = labels.T[0]\n",
    "    loss = loss_fcn(output, labels)\n",
    "    predict = output.argmax(dim=1)\n",
    "    score = (labels == predict).sum().item() / len(labels)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=174, num_edges=882,\n",
       "       ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
       "       edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}),\n",
       " tensor([[3],\n",
       "         [0],\n",
       "         [0],\n",
       "         [5],\n",
       "         [3]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataloader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.2191\n",
      "Accuracy score: 0.2167\n",
      "Epoch 5 | Loss: 1.6944\n",
      "Accuracy score: 0.2750\n",
      "Epoch 10 | Loss: 1.6418\n",
      "Accuracy score: 0.2750\n",
      "Epoch 15 | Loss: 1.6025\n",
      "Accuracy score: 0.2750\n",
      "Epoch 20 | Loss: 1.5811\n",
      "Accuracy score: 0.3083\n",
      "Epoch 25 | Loss: 1.5498\n",
      "Accuracy score: 0.3333\n",
      "Epoch 30 | Loss: 1.5137\n",
      "Accuracy score: 0.3417\n",
      "Epoch 35 | Loss: 1.4738\n",
      "Accuracy score: 0.3833\n",
      "Epoch 40 | Loss: 1.4339\n",
      "Accuracy score: 0.4167\n",
      "Epoch 45 | Loss: 1.3850\n",
      "Accuracy score: 0.4667\n",
      "Epoch 50 | Loss: 1.3271\n",
      "Accuracy score: 0.4750\n",
      "Epoch 55 | Loss: 1.2644\n",
      "Accuracy score: 0.4833\n",
      "Epoch 60 | Loss: 1.2146\n",
      "Accuracy score: 0.4667\n",
      "Epoch 65 | Loss: 1.1823\n",
      "Accuracy score: 0.4833\n",
      "Epoch 70 | Loss: 1.1199\n",
      "Accuracy score: 0.5333\n",
      "Epoch 75 | Loss: 1.0671\n",
      "Accuracy score: 0.4833\n",
      "Epoch 80 | Loss: 1.0408\n",
      "Accuracy score: 0.4750\n",
      "Epoch 85 | Loss: 0.9944\n",
      "Accuracy score: 0.4583\n",
      "Epoch 90 | Loss: 0.9759\n",
      "Accuracy score: 0.4583\n",
      "Epoch 95 | Loss: 0.9195\n",
      "Accuracy score: 0.4000\n",
      "Epoch 100 | Loss: 1.0335\n",
      "Accuracy score: 0.4917\n",
      "Epoch 105 | Loss: 0.8412\n",
      "Accuracy score: 0.5000\n",
      "Epoch 110 | Loss: 0.8184\n",
      "Accuracy score: 0.4667\n",
      "Epoch 115 | Loss: 0.9278\n",
      "Accuracy score: 0.4333\n",
      "Epoch 120 | Loss: 0.7562\n",
      "Accuracy score: 0.4250\n",
      "Epoch 125 | Loss: 0.7478\n",
      "Accuracy score: 0.4583\n",
      "Epoch 130 | Loss: 0.6718\n",
      "Accuracy score: 0.4750\n",
      "Epoch 135 | Loss: 0.6835\n",
      "Accuracy score: 0.4583\n",
      "Epoch 140 | Loss: 0.6242\n",
      "Accuracy score: 0.4667\n",
      "Epoch 145 | Loss: 0.5729\n",
      "Accuracy score: 0.4333\n",
      "Accuracy score: 0.4500\n"
     ]
    }
   ],
   "source": [
    "# Store features\n",
    "n_features, n_classes = dataset[0][0].ndata['node_attr'].shape[1], \\\n",
    "    dataset.num_labels\n",
    "hidden_size = 64\n",
    "\n",
    "# Define model, loss function and optimizer\n",
    "model = BasicGraphModel(n_layers=3, input_size=n_features,\n",
    "                        hidden_size=hidden_size, output_size=n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and test\n",
    "train(model, loss_fcn, optimizer,\n",
    "        train_dataloader, val_dataloader, num_epochs=150)\n",
    "test(model, loss_fcn, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specformer",
   "language": "python",
   "name": "specformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
