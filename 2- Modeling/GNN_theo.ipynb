{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx.algorithms.community as community\n",
    "import torch\n",
    "\n",
    "# Import packages\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import GraphConv\n",
    "from IPython.display import Latex\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"../data/test_set_final.csv\")\n",
    "train_set = pd.read_csv(\"../data/train_set_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the 1 to 932 column named number_source into a single column as an array of values\n",
    "train_set['node_info_source'] = train_set[train_set.columns[18:950]].values.tolist()\n",
    "train_set.drop(train_set.columns[18:950], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['node_info_target'] = train_set[train_set.columns[18:950]].values.tolist()\n",
    "train_set.drop(train_set.columns[18:950], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'node_info_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'node_info_source'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m graph \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39madd_self_loop(graph)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert 'node_info_source' and 'node_info_target' to PyTorch tensors and stack them\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m node_info_source_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode_info_source\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x))\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      9\u001b[0m node_info_target_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_info_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x))\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Drop the 'source', 'target', 'label', 'node_info_source', and 'node_info_target' columns and convert the rest to a PyTorch tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'node_info_source'"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "# Create a graph\n",
    "# Create a graph from the training set\n",
    "graph = dgl.graph((train_set['source'], train_set['target']), num_nodes=len(train_set))\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "# Convert 'node_info_source' and 'node_info_target' to PyTorch tensors and stack them\n",
    "node_info_source_tensor = torch.stack(train_set['node_info_source'].apply(lambda x: torch.tensor(x)).tolist())\n",
    "node_info_target_tensor = torch.stack(train_set['node_info_target'].apply(lambda x: torch.tensor(x)).tolist())\n",
    "\n",
    "# Drop the 'source', 'target', 'label', 'node_info_source', and 'node_info_target' columns and convert the rest to a PyTorch tensor\n",
    "features_tensor = torch.tensor(train_set.drop(['source', 'target', 'label', 'node_info_source', 'node_info_target'], axis=1).values.astype(float))\n",
    "\n",
    "# Concatenate the tensors along the second dimension\n",
    "features = torch.cat([features_tensor, node_info_source_tensor, node_info_target_tensor], dim=1)\n",
    "\n",
    "# Assign the features to the graph\n",
    "graph.ndata['features'] = features\n",
    "\n",
    "# Set the edge labels\n",
    "graph.edata['label'] = torch.tensor(train_set['label'])\n",
    "\n",
    "# Print the graph information\n",
    "print(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node labels and features\n",
      "{'label': tensor([1, 1, 1,  ..., 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print('Node labels and features')\n",
    "print(graph.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  1879\n",
      "Number of Nodes:  10496\n"
     ]
    }
   ],
   "source": [
    "# Define key graph variables\n",
    "X = graph.ndata['features']\n",
    "y = graph.edata['label']\n",
    "num_classes = 2\n",
    "num_feat = X.shape[1]\n",
    "N = graph.number_of_nodes()\n",
    "\n",
    "print('Number of features: ', num_feat)\n",
    "print('Number of Nodes: ', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(N, train_ratio, seed=4):\n",
    "    \"\"\" Creates train/val/test masks\n",
    "\n",
    "    Args:\n",
    "        N (int): dataset size\n",
    "        train_ratio (float): proportion of the training set\n",
    "        seed (int, optional): Fixes random. Defaults to 10\n",
    "\n",
    "    Return: \n",
    "        [tensors]: returns boolean tensors for train/val/test set\n",
    "        True indicates that a node belong to this set, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(train_ratio * N)\n",
    "    val_size = int((N - train_size)/2)\n",
    "    test_size = N - train_size - val_size\n",
    "\n",
    "    # split dataset\n",
    "    subsets = torch.utils.data.random_split(range(N), lengths = [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "    train_inds, val_inds, test_inds = [torch.Tensor(subset.indices) for subset in subsets]\n",
    "\n",
    "    # create tensors of masks for each subset\n",
    "    dataset_inds = torch.arange(N)\n",
    "    train_mask = torch.isin(dataset_inds, train_inds)\n",
    "    val_mask = torch.isin(dataset_inds, val_inds)\n",
    "    test_mask = torch.isin(dataset_inds, test_inds)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "train_mask, val_mask, test_mask = split_dataset(N, train_ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Graph Convolution Network \n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_size, hidden_size, output_size, dropout):\n",
    "        super(GNN_model, self).__init__()\n",
    "\n",
    "        # Define GNN components\n",
    "        self.convs = torch.nn.ModuleList() # holds GraphConv layers in a list\n",
    "        self.convs.append(\n",
    "            GraphConv(input_size, hidden_size, activation=F.relu)) # You can either define the activation at the layer level or call it inside the forward\n",
    "        for i in range(num_layers-2):\n",
    "            self.convs.append(\n",
    "                GraphConv(hidden_size, hidden_size, activation=F.relu))\n",
    "        self.convs.append(GraphConv(hidden_size, output_size))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        # Implement the forward function that takes the graph,\n",
    "        # the features tensor x and returns the output tensor as shown in figure 1\n",
    "        for conv in self.convs:\n",
    "            x = conv(graph, x)\n",
    "    \n",
    "        output = F.log_softmax(x, dim=1) # Log_softmax is more stable numerically in comparison to softmax\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, x, labels, num_epochs, optimizer, train_mask, val_mask, test_mask):\n",
    "    \"\"\" Train the GNN model \n",
    "\n",
    "    Args:\n",
    "        model: GNN model defined in pytorch\n",
    "        graph (dgl.graph): dataset on which the task is performed\n",
    "        x (tensor): node feature matrix \n",
    "        labels (tensor): node labels\n",
    "        num_epochs (int): number of epochs\n",
    "        train_mask (tensor): boolean mask for training nodes\n",
    "        val_mask (tensor): boolean mask for validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the model (pytorch specific)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward\n",
    "        pred = model(graph, x)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = nll_loss(pred[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}: loss {:.3f}, train Acc: {:.3f}, val acc: {:.3f}, test acc: {:.3f}'.format(\n",
    "                epoch, loss, train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate model\n",
    "num_layers=3\n",
    "hidden_size=16\n",
    "dropout=0.3\n",
    "num_epochs=300\n",
    "lr=0.01\n",
    "weight_decay=0.005\n",
    "train_ratio=0.8\n",
    "seed=4\n",
    "\n",
    "model = GNN_model(num_layers, num_feat, hidden_size, num_classes, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.778, train Acc: 0.200, val acc: 0.900, test acc: 0.900\n",
      "Epoch 10: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 20: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 30: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 40: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 50: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 60: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 70: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 80: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 90: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 100: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 110: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 120: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 130: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 140: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 150: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 160: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 170: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 180: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 190: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 200: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 210: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 220: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 230: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 240: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 250: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 260: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 270: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 280: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n",
      "Epoch 290: loss 0.000, train Acc: 1.000, val acc: 1.000, test acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "X = X.float()\n",
    "y = y.long()\n",
    "train_mask = train_mask.long()\n",
    "val_mask = val_mask.long()\n",
    "test_mask = test_mask.long()\n",
    "\n",
    "# Train model\n",
    "train(model, graph, X, y, num_epochs, optimizer, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# test Model sur le test set \n",
    "model.eval()\n",
    "pred = model(graph, X)\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "test_acc = (pred[test_mask] == y[test_mask]).float().mean()\n",
    "print('Test accuracy:', test_acc.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
