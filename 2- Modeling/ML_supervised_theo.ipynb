{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx.algorithms.community as community\n",
    "import torch\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Machine Learning in Network Science\n",
    "Lab 2: Link prediction\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Import packages\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import GraphConv\n",
    "from IPython.display import Latex\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"../data/test_set_final.csv\")\n",
    "train_set = pd.read_csv(\"../data/train_set_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the 1 to 932 column named number_source into a single column as an array of values\n",
    "train_set['node_info_source'] = train_set[train_set.columns[18:950]].values.tolist()\n",
    "train_set.drop(train_set.columns[18:950], axis=1, inplace=True)\n",
    "train_set['node_info_target'] = train_set[train_set.columns[18:950]].values.tolist()\n",
    "train_set.drop(train_set.columns[18:950], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(train_set, 'source', 'target', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(graph, train_features, test_features, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Downstream ML task using edge embeddings to classify them \n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Build the model and train it ---\n",
    "    # Fill in the blanks\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_features, train_labels)\n",
    "\n",
    "    train_preds = clf.predict_proba(train_features)[:, 1]\n",
    "    test_preds = clf.predict_proba(test_features)[:, 1]\n",
    "\n",
    "    # --- Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from predictions ---\n",
    "    # Fill in the blanks\n",
    "    fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, color='darkred', label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='lightgray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(graph, samples):\n",
    "    \"\"\"\n",
    "    Creates a feature vector for each edge of the graph contained in samples \n",
    "    \"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    # --- Extract manually diverse features relative to each edge contained in samples --- \n",
    "    # Fill in the blanks\n",
    "\n",
    "    # Degree Centrality measure\n",
    "    deg_centrality = nx.degree_centrality(graph)\n",
    "    \n",
    "    # Betweeness centrality measure\n",
    "    betweeness_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "    for edge in tqdm(samples):\n",
    "        source_node, target_node = edge[0], edge[1]\n",
    "\n",
    "        # Degree Centrality\n",
    "        source_degree_centrality = deg_centrality[source_node]\n",
    "        target_degree_centrality = deg_centrality[target_node]\n",
    "        \n",
    "        # Betweeness centrality measure \n",
    "        diff_bt = betweeness_centrality[target_node] - betweeness_centrality[source_node]\n",
    "\n",
    "        # Preferential Attachement \n",
    "        pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n",
    "\n",
    "        # AdamicAdar\n",
    "        aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n",
    "\n",
    "        # Jaccard\n",
    "        jacard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n",
    "        \n",
    "        # Create edge feature vector with all metric computed above\n",
    "        feature_vector.append(np.array([source_degree_centrality, target_degree_centrality, \n",
    "                                        diff_bt, pref_attach, aai, jacard_coeff]) ) \n",
    "        \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(graph, train_set_ratio):\n",
    "    \"\"\"\n",
    "    Graph pre-processing step required to perform supervised link prediction\n",
    "    Create training and test sets\n",
    "    \"\"\"\n",
    "        \n",
    "    # --- Step 0: The graph must be connected ---\n",
    "    if nx.is_connected(G) is not True:\n",
    "        raise ValueError(\"The graph contains more than one connected component!\")\n",
    "       \n",
    "    # --- Step 1: Generate positive edge samples for testing set ---\n",
    "    residual_g = graph.copy()\n",
    "    test_pos_samples = []\n",
    "      \n",
    "    # Store the shuffled list of current edges of the graph\n",
    "    edges = list(residual_g.edges())\n",
    "    np.random.shuffle(edges)\n",
    "    \n",
    "    # Define number of positive test samples desired\n",
    "    test_set_size = int((1.0 - train_set_ratio) * graph.number_of_edges())\n",
    "    train_set_size = graph.number_of_edges() - test_set_size\n",
    "    num_of_pos_test_samples = 0\n",
    "    \n",
    "    # Remove random edges from the graph, leaving it connected\n",
    "    # Fill in the blanks\n",
    "    for edge in edges:\n",
    "        \n",
    "        # Remove the edge\n",
    "        residual_g.remove_edge(edge[0], edge[1])\n",
    "        \n",
    "        # Add the removed edge to the positive sample list if the network is still connected\n",
    "        if nx.is_connected(residual_g):\n",
    "            num_of_pos_test_samples += 1\n",
    "            test_pos_samples.append(edge)\n",
    "        # Otherwise, re-add the edge to the network\n",
    "        else: \n",
    "            residual_g.add_edge(edge[0], edge[1])\n",
    "        \n",
    "        # If we have collected enough number of edges for testing set, we can terminate the loop\n",
    "        if num_of_pos_test_samples == test_set_size:\n",
    "            break\n",
    "    \n",
    "    # Check if we have the desired number of positive samples for testing set \n",
    "    if num_of_pos_test_samples != test_set_size:\n",
    "        raise ValueError(\"Enough positive edge samples could not be found!\")\n",
    "\n",
    "        \n",
    "    # --- Step 2: Generate positive edge samples for training set ---\n",
    "    # The remaining edges are simply considered for positive samples of the training set\n",
    "    train_pos_samples = list(residual_g.edges())\n",
    "        \n",
    "        \n",
    "    # --- Step 3: Generate the negative samples for testing and training sets ---\n",
    "    # Fill in the blanks\n",
    "    non_edges = list(nx.non_edges(graph))\n",
    "    np.random.shuffle(non_edges)\n",
    "    \n",
    "    train_neg_samples = non_edges[:train_set_size] \n",
    "    test_neg_samples = non_edges[train_set_size:train_set_size + test_set_size]\n",
    "\n",
    "    \n",
    "    # --- Step 4: Combine sample lists and create corresponding labels ---\n",
    "    # For training set\n",
    "    train_samples = train_pos_samples + train_neg_samples\n",
    "    train_labels = [1 for _ in train_pos_samples] + [0 for _ in train_neg_samples]\n",
    "    # For testing set\n",
    "    test_samples = test_pos_samples + test_neg_samples\n",
    "    test_labels = [1 for _ in test_pos_samples] + [0 for _ in test_neg_samples]\n",
    "    \n",
    "    return residual_g, train_samples, train_labels, test_samples, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m residual_g, train_samples, train_labels, test_samples, test_labels \u001b[38;5;241m=\u001b[39m generate_samples(graph\u001b[38;5;241m=\u001b[39mG, train_set_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# --- Create feature vector for all edges in training set and test set ---\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:, \u001b[38;5;241m3\u001b[39m:]\n\u001b[1;32m      6\u001b[0m test_features \u001b[38;5;241m=\u001b[39m test_samples\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# --- Link prediction ---\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# --- Construct the training and testing sets ---\n",
    "residual_g, train_samples, train_labels, test_samples, test_labels = generate_samples(graph=G, train_set_ratio=0.6)\n",
    "\n",
    "# --- Create feature vector for all edges in training set and test set ---\n",
    "train_features = train_set.iloc[:, 3:]\n",
    "test_features = test_set.iloc[:, 2:]\n",
    "train_labels = train_set['label']\n",
    "\n",
    "# --- Link prediction ---\n",
    "prediction(G, train_features, test_features, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
