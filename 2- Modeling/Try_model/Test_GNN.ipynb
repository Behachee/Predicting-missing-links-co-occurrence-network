{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import itertools\n",
    "\n",
    "from dgl.nn import SAGEConv\n",
    "import scipy.sparse as sp\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_final = pd.read_csv(\"../data/test_set_final.csv\")\n",
    "train_set_final = pd.read_csv(\"../data/train_set_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = train_set_final[train_set_final['label'] == 1][['source', 'target']].values.tolist()\n",
    "\n",
    "graph = nx.Graph()\n",
    "graph.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "degrees = [graph.degree(n) for n in graph.nodes()]\n",
    "\n",
    "isolated_nodes = degrees == 0\n",
    "\n",
    "# Convert the isolated_nodes variable to a tensor\n",
    "isolated_nodes = torch.tensor(isolated_nodes)\n",
    "\n",
    "# Get the indices of the isolated nodes\n",
    "isolated_nodes = isolated_nodes.nonzero().flatten()\n",
    "\n",
    "# Print the indices of the isolated nodes\n",
    "print(isolated_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m u, v \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medges()\n\u001b[1;32m      5\u001b[0m eids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(graph\u001b[38;5;241m.\u001b[39mnumber_of_edges())\n\u001b[1;32m      6\u001b[0m eids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(eids)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "u, v = graph.edges()\n",
    "\n",
    "eids = np.arange(graph.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.1)\n",
    "train_size = graph.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "num_nodes = graph.number_of_nodes()\n",
    "\n",
    "# Create the adjacency matrix with an explicit shape\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())), shape=(num_nodes, num_nodes))\n",
    "\n",
    "# Now, when subtracting from a dense matrix of ones and the identity matrix, shapes will match\n",
    "adj_neg = 1 - adj.todense() - np.eye(num_nodes)\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "# The rest of your code for selecting negative edges can remain the same\n",
    "neg_eids = np.random.choice(len(neg_u), graph.number_of_edges())\n",
    "\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(graph, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE model for node classification.\n",
    "\n",
    "    Args:\n",
    "        in_feats (int): Number of input features.\n",
    "        h_feats (int): Number of hidden features.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (SAGEConv): First GraphSAGE convolutional layer.\n",
    "        conv2 (SAGEConv): Second GraphSAGE convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphSAGE model.\n",
    "\n",
    "        Args:\n",
    "            g (DGLGraph): Input graph.\n",
    "            in_feat (torch.Tensor): Input node features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output node representations.\n",
    "        \"\"\"\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    DotPredictor is a PyTorch module that computes a dot product between the source node feature 'h' and \n",
    "    destination node feature 'h' for each edge in the input graph 'g'. It returns the computed scores as \n",
    "    edge features.\n",
    "    \"\"\"\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        \"\"\"\n",
    "        Initializes the MLP Predictor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_feats : int\n",
    "            The number of input features for each node.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        \"\"\"\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges :\n",
    "            Has three members ``src``, ``dst`` and ``data``, each of\n",
    "            which is a dictionary representing the features of the\n",
    "            source nodes, the destination nodes, and the edges\n",
    "            themselves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features.\n",
    "\n",
    "        \"\"\"\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the MLP Predictor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : dgl.DGLGraph\n",
    "            The input graph.\n",
    "        h : torch.Tensor\n",
    "            The input node features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The predicted scores for each edge in the graph.\n",
    "\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "#pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    \"\"\"\n",
    "    Compute the binary cross entropy loss given positive and negative scores.\n",
    "\n",
    "    Args:\n",
    "        pos_score (torch.Tensor): Tensor containing the positive scores.\n",
    "        neg_score (torch.Tensor): Tensor containing the negative scores.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed binary cross entropy loss.\n",
    "    \"\"\"\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    \"\"\"\n",
    "    Compute the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) score.\n",
    "\n",
    "    Args:\n",
    "        pos_score (torch.Tensor): Tensor containing the positive scores.\n",
    "        neg_score (torch.Tensor): Tensor containing the negative scores.\n",
    "\n",
    "    Returns:\n",
    "        float: The AUC-ROC score.\n",
    "\n",
    "    \"\"\"\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "all_logits = []\n",
    "for e in range(400):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata['feat'])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network (GCN) class.\n",
    "\n",
    "    This class implements a two-layer GCN model for node classification.\n",
    "\n",
    "    Parameters:\n",
    "    - in_feats (int): Number of input features.\n",
    "    - h_feats (int): Number of hidden features.\n",
    "    - num_classes (int): Number of output classes.\n",
    "\n",
    "    Attributes:\n",
    "    - conv1 (GraphConv): First graph convolutional layer.\n",
    "    - conv2 (GraphConv): Second graph convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN model.\n",
    "\n",
    "        Parameters:\n",
    "        - g (DGLGraph): Input graph.\n",
    "        - in_feat (torch.Tensor): Input features.\n",
    "\n",
    "        Returns:\n",
    "        - h (torch.Tensor): Output features.\n",
    "        \"\"\"\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "in_feats = graph.ndata['feat'].shape[1]\n",
    "h_feats = 16  # Hidden layer size\n",
    "num_classes = 2  # Assuming binary classification for link prediction\n",
    "\n",
    "model = GCN(in_feats, h_feats, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy loss given positive and negative scores.\n",
    "\n",
    "    Args:\n",
    "        pos_score (torch.Tensor): Tensor containing the positive scores.\n",
    "        neg_score (torch.Tensor): Tensor containing the negative scores.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.size(0)), torch.zeros(neg_score.size(0))])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def train(model, g, features, train_pos_edges, train_neg_edges, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the given model using the provided graph, features, positive edges, negative edges, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained.\n",
    "        g: The graph used for training.\n",
    "        features: The features used for training.\n",
    "        train_pos_edges: The positive edges used for training.\n",
    "        train_neg_edges: The negative edges used for training.\n",
    "        optimizer: The optimizer used for training.\n",
    "\n",
    "    Returns:\n",
    "        The loss value as a float.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(g, features)\n",
    "    pos_score = (embeddings[train_pos_edges[:, 0]] * embeddings[train_pos_edges[:, 1]]).sum(dim=1)\n",
    "    neg_score = (embeddings[train_neg_edges[:, 0]] * embeddings[train_neg_edges[:, 1]]).sum(dim=1)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset and graph are ready\n",
    "features = graph.ndata['feat']\n",
    "in_feats, h_feats, out_feats = features.shape[1], 16, 16\n",
    "\n",
    "model = GCN(in_feats, h_feats, out_feats)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    loss = train(model, graph, features, train_pos_edges, train_neg_edges, optimizer)\n",
    "    print(f'Epoch {epoch}: Loss {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predicted_links, columns=['Predicted'])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "predictions_df.to_csv('GCN_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network (GAT) model.\n",
    "\n",
    "    Args:\n",
    "        num_node_features (int): Number of input node features.\n",
    "        hidden_channels (int): Number of hidden channels in the GAT layers.\n",
    "        out_features (int): Number of output features.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (torch_geometric.nn.conv.GATConv): First GAT convolutional layer.\n",
    "        conv2 (torch_geometric.nn.conv.GATConv): Second GAT convolutional layer.\n",
    "        conv3 (torch_geometric.nn.conv.GATConv): Third GAT convolutional layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_node_features, hidden_channels, out_features):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=4, dropout=0.2)\n",
    "        # Concatenation will happen on the output features so the dimension will increase\n",
    "        self.conv2 = GATConv(hidden_channels * 4, hidden_channels, heads=4, concat=True, dropout=0.2)\n",
    "        # Since concat is set to True, the input features of the next layer are the hidden_channels * num_heads\n",
    "        self.conv3 = GATConv(hidden_channels * 4, out_features, concat=False, heads=1, dropout=0.2)  # No concatenation in the final layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features.\n",
    "            edge_index (torch.Tensor): Graph edge indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)  # Add dropout for regularization\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)  # Add dropout for regularization\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.sigmoid(x.squeeze())\n",
    "\n",
    "model = GAT(num_node_features=data.num_node_features, hidden_channels=64, out_features=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    test_loss, auc_score = test()\n",
    "    print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, AUC: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    test_pred = model(data.x, test_edge_index).squeeze()\n",
    "\n",
    "# Convert predictions to probabilities using a sigmoid function (if not already done within the model)\n",
    "test_pred_prob = torch.sigmoid(test_pred)\n",
    "\n",
    "# You can set a threshold to classify edges as existing or not, e.g., threshold = 0.5\n",
    "threshold = 0.5\n",
    "predicted_links = (test_pred_prob >= threshold).int()\n",
    "\n",
    "# Print or save your predictions\n",
    "print(predicted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
