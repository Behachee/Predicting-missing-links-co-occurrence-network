{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_columns', None)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx.algorithms.community as nx_community\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_info = pd.read_csv(\"/Users/jojolapatate/Documents/GitHub/Predicting-missing-links-co-occurrence-network/data/node_information.csv\", header=None)\n",
    "test_set = pd.read_csv(\"/Users/jojolapatate/Documents/GitHub/Predicting-missing-links-co-occurrence-network/data/test.txt\", sep=\" \", header=None, names=['source', 'target'])\n",
    "train_set = pd.read_csv(\"/Users/jojolapatate/Documents/GitHub/Predicting-missing-links-co-occurrence-network/data/train.txt\", sep=\" \", header=None, names=['source', 'target', 'label'])\n",
    "\n",
    "# Graph creation\n",
    "G = nx.from_pandas_edgelist(train_set, 'source', 'target', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salton_similarity(G, edges):\n",
    "    for u, v in edges:\n",
    "        common_neighbors = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degree_u = G.degree(u)\n",
    "        degree_v = G.degree(v)\n",
    "        yield u, v, common_neighbors / ((degree_u * degree_v) ** 0.5)\n",
    "\n",
    "def sorenson_similarity(G, edges):\n",
    "    for u, v in edges:\n",
    "        common_neighbors = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degree_u = G.degree(u)\n",
    "        degree_v = G.degree(v)\n",
    "        yield u, v, 2 * common_neighbors / (degree_u + degree_v)\n",
    "\n",
    "\n",
    "def hub_promoted_similarity(G, edges):\n",
    "    for u, v in edges:\n",
    "        common_neighbors = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degree_u = G.degree(u)\n",
    "        degree_v = G.degree(v)\n",
    "        yield u, v, common_neighbors / min(degree_u, degree_v)\n",
    "\n",
    "def hub_depressed_similarity(G, edges):\n",
    "    for u, v in edges:\n",
    "        common_neighbors = len(list(nx.common_neighbors(G, u, v)))\n",
    "        degree_u = G.degree(u)\n",
    "        degree_v = G.degree(v)\n",
    "        yield u, v, common_neighbors / max(degree_u, degree_v)\n",
    "\n",
    "def adamic_adar_index(G, edges):\n",
    "    for u, v in edges:\n",
    "        score = 0\n",
    "        for w in nx.common_neighbors(G, u, v):\n",
    "            degree_w = G.degree(w)\n",
    "            if degree_w > 1:\n",
    "                score += 1 / np.log(degree_w)\n",
    "        yield u, v, score\n",
    "\n",
    "# Creer une fonction qui ajoute des features (degree, centrality, clustering) à un graphes et qui merge node_info avec le graphe\n",
    "def add_node_attributes(df, node_info):\n",
    "     # Créer un graphe à partir du df\n",
    "     if 'label' in df.columns:\n",
    "        G = nx.from_pandas_edgelist(df, 'source', 'target', 'label')\n",
    "     else:\n",
    "        G = nx.from_pandas_edgelist(df, 'source', 'target')\n",
    "\n",
    "     # Calculer les caractéristiques\n",
    "     df['degree_source'] = df['source'].apply(lambda x: G.degree(x))\n",
    "     df['centrality_source'] = df['source'].apply(lambda x: nx.degree_centrality(G)[x])\n",
    "     #df['clustering_source'] = df['source'].apply(lambda x: nx.clustering(G)[x])\n",
    "\n",
    "     df['degree_target'] = df['target'].apply(lambda x: G.degree(x))\n",
    "     df['centrality_target'] = df['target'].apply(lambda x: nx.degree_centrality(G)[x])\n",
    "     #df['clustering_target'] = df['target'].apply(lambda x: nx.clustering(G)[x])\n",
    "\n",
    "     # Détecter les communautés et créer une caractéristique de communauté\n",
    "     communities = nx_community.greedy_modularity_communities(G)\n",
    "     community_map = {}\n",
    "     for i, community in enumerate(communities):\n",
    "          for node in community:\n",
    "               community_map[node] = i\n",
    "     df['community_source'] = df['source'].apply(lambda x: community_map[x])\n",
    "     df['community_target'] = df['target'].apply(lambda x: community_map[x])\n",
    "\n",
    "     # Calculer le coefficient de Jaccard\n",
    "     df['jaccard'] = [i[2] for i in nx.jaccard_coefficient(G, df[['source', 'target']].values)]\n",
    "\n",
    "     # Calculer le coefficient de similarité de Salton\n",
    "     df['salton'] = [i[2] for i in salton_similarity(G, df[['source', 'target']].values.tolist())]\n",
    "\n",
    "     # Calculer le coefficient de similarité de Sorenson\n",
    "     df['sorenson'] = [i[2] for i in sorenson_similarity(G, df[['source', 'target']].values.tolist())]\n",
    "\n",
    "     # Calculer le coefficient de similarité de Hub Promoted\n",
    "     df['hub_promoted'] = [i[2] for i in hub_promoted_similarity(G, df[['source', 'target']].values.tolist())]\n",
    "\n",
    "     # Calculer le coefficient de similarité de Hub Depressed\n",
    "     df['hub_depressed'] = [i[2] for i in hub_depressed_similarity(G, df[['source', 'target']].values.tolist())]\n",
    "\n",
    "     # Calculer le coefficient de similarité de Leicht-Holme-Newman\n",
    "     df['leicht_holme_newman'] = [i[2] for i in nx.preferential_attachment(G, df[['source', 'target']].values)]\n",
    "\n",
    "     # Calculer le coefficient de similarité de adamic_adar\n",
    "     df['adamic_adar'] = [i[2] for i in adamic_adar_index(G, df[['source', 'target']].values.tolist())]\n",
    "\n",
    "     # Calculer le coefficient de similarité de resource_allocation_index\n",
    "     df['resource_allocation'] = [i[2] for i in nx.resource_allocation_index(G, df[['source', 'target']].values)]\n",
    "\n",
    "     # Calculer les common neighbors\n",
    "     df['common_neighbors'] = df.apply(lambda x: len(list(nx.common_neighbors(G, x['source'], x['target']))), axis=1)\n",
    "\n",
    "     df['resource_allocation'] = [i[2] for i in nx.resource_allocation_index(G, df[['source', 'target']].values)]\n",
    "\n",
    "     # Calculer les common neighbors\n",
    "     df['common_neighbors'] = df.apply(lambda x: len(list(nx.common_neighbors(G, x['source'], x['target']))), axis=1)\n",
    "    \n",
    "     shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "     df['shortest_path_length'] = [shortest_paths.get((source, target), -1) for source, target in zip(df['source'], df['target'])]\n",
    "\n",
    "    # Calculate eigenvector centrality\n",
    "     eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
    "     df['eigenvector_centrality_source'] = df['source'].apply(lambda x: eigenvector_centrality.get(x, 0))\n",
    "     df['eigenvector_centrality_target'] = df['target'].apply(lambda x: eigenvector_centrality.get(x, 0))\n",
    "\n",
    "     # Fusionner node_info avec le df\n",
    "     node_info.rename(columns={0: 'node_id'}, inplace=True)\n",
    "     df = df.merge(node_info, left_on='source', right_on='node_id', how='left')\n",
    "     # rename toutes les colonnes de node_info ajouter par \"nom_col\"+\"source\"\n",
    "     df.rename(columns={col: str(col) + '_source' for col in node_info.columns[1:]}, inplace=True)\n",
    "     df.drop('node_id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "     df = df.merge(node_info, left_on='target', right_on='node_id', how='left')\n",
    "     # rename toutes les colonnes de node_info ajouter par \"nom_col\"+\"source\"\n",
    "     df.rename(columns={col: str(col) + '_target' for col in node_info.columns[1:]}, inplace=True)\n",
    "     df.drop('node_id', axis=1, inplace=True)\n",
    "     return df\n",
    "\n",
    "# Ajouter les caractéristiques au train_set\n",
    "train_set = add_node_attributes(train_set, node_info)\n",
    "test_set = add_node_attributes(test_set, node_info)\n",
    "\n",
    "train_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('../data/train_set_final.csv', index=False)\n",
    "test_set.to_csv('../data/test_set_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
